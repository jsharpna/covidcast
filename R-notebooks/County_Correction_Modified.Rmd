---
title: "County usa-facts confirmed case corrections with backfilling"
author: "Delphi Group"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    code_folding: hide
params:
  window_size: 14
  start_date: NULL
  sig_cut: 3
  size_cut: 20
  sig_consec: 2.25
  outlier_start_date: "2020-03-01"
  cache_data: TRUE
  backfill_lag: 14
  excess_cut: 0
  write_RDS: TRUE
---

```{r, message=FALSE, warning=FALSE}
library(covidcast)
library(dplyr)
library(tidyr)
library(lubridate)
library(RcppRoll)
library(cowplot)
library(ggplot2)
library(DT)
library(ggforce)
library(ggpubr)
library(egg)
knitr::opts_chunk$set(warning = FALSE, message=FALSE)
library(grid)
source("corrections-funs.R")
attach(params)

```

Data retrieval
```{r, cache=params$cache_data}
counties <-  suppressMessages(
  covidcast_signal(
  "usa-facts","confirmed_incidence_num", 
  geo_type = "county", 
  start_day = "2020-03-01")
)
```


Data Wrangling notes:
* Only counties with > 30 days' available data will be selected
* Counties whose maximum daily_confirmed_case <10 are filter out
* Top 250 of counties sorted by total cases will be selected
* 
```{r}
todump <- counties %>% group_by(geo_value) %>% 
  summarise(
    ava_value_count = sum(!is.na(value)),
    case_sum = sum(value,na.rm = T),
    max_case = max(value)
    ) %>% 
  filter(max_case>=10, 
         ava_value_count>=30, 
         as.numeric(geo_value) %% 1000 > 0) %>% 
  arrange(desc(case_sum)) %>% top_n(250, wt=case_sum) 

county_filtered <- subset(counties,counties$geo_value %in% todump$geo_value)
```





```{r calculate-roll-stats}
county_filtered <- county_filtered %>% group_by(geo_value)  %>% mutate(
  fmean = roll_meanr(value, window_size),
  smean = roll_mean(value, window_size, fill = NA),
  fmedian = roll_medianr(value, window_size),
  smedian = roll_median(value, window_size, fill = NA),
  fsd = roll_sdr(value, window_size),
  ssd = roll_sd(value, window_size,fill = NA),
  fmad = roll_medianr(abs(value-fmedian), window_size,na.rm=TRUE),
  smad = roll_median(abs(value-smedian), na.rm=TRUE),
  ftstat = abs(value-fmedian)/fsd, # mad in denominator is wrong scale, 
  ststat = abs(value-smedian)/ssd, # basically results in all the data flagged
  flag = 
    (abs(value) > size_cut & !is.na(ststat) & ststat > sig_cut) | # best case
    (is.na(ststat) & abs(value) > size_cut & !is.na(ftstat) & ftstat > sig_cut) | 
      # use filter if smoother is missing
    (value < -size_cut & !is.na(ststat) & !is.na(ftstat)), # big negative
    #(fmean > 10 & fmean< 20 & value > 2*sig_cut*fmean)
  flag = flag | # these allow smaller values to also be outliers if they are consecutive
    (dplyr::lead(flag) & !is.na(ststat) & ststat > sig_consec) | 
    (dplyr::lag(flag) & !is.na(ststat) & ststat > sig_consec) |
    (dplyr::lead(flag) & is.na(ststat) & ftstat > sig_consec) |
    (dplyr::lag(flag) & is.na(ststat) & ftstat > sig_consec),
  RI_daily_flag = (geo_value=="44007" & value!=0 & 
                     lead(value,n=1L)!=0 & lead(value,n=2L)!=0 
                   & lead(value,n=3L)!=0)
  )


#Mapping counties to states

STATE_TO_FIPS = c( ## copied from somewhere in the bowels of the code
  'WA'='53', 'DE'='10', 'DC'='11', 'WI'='55', 'WV'='54', 'HI'='15',
  'FL'='12', 'WY'='56', 'PR'='72', 'NJ'='34', 'NM'='35', 'TX'='48',
  'LA'='22', 'NC'='37', 'ND'='38', 'NE'='31', 'TN'='47', 'NY'='36',
  'PA'='42', 'AK'='02', 'NV'='32', 'NH'='33', 'VA'='51', 'CO'='08',
  'CA'='06', 'AL'='01', 'AR'='05', 'VT'='50', 'IL'='17', 'GA'='13',
  'IN'='18', 'IA'='19', 'MA'='25', 'AZ'='04', 'ID'='16', 'CT'='09',
  'ME'='23', 'MD'='24', 'OK'='40', 'OH'='39', 'UT'='49', 'MO'='29',
  'MN'='27', 'MI'='26', 'RI'='44', 'KS'='20', 'MT'='30', 'MS'='28',
  'SC'='45', 'KY'='21', 'OR'='41', 'SD'='46',
  'AS'='60', 'GU'='66', 'MP'='69', 'VI'='78', 'UM'='74'
)

county_filtered <-  county_filtered %>% 
  mutate(FIP = substr(geo_value,1,2)) %>% 
  mutate(state = names(STATE_TO_FIPS)[match(FIP,STATE_TO_FIPS)]) %>% 
  select(-FIP) %>% relocate(state,.after=geo_value)

```

## visualize the outliers
```{r fig.height = 50, fig.width = 15}


df <- county_filtered %>% unite(county,geo_value:state,sep = "_")
df$county <- as.factor(df$county)

  p <- ggplot(df %>% group_by(county)%>% filter(any(flag == 'TRUE')), aes(x=time_value, y=value))+
        geom_line(aes(color="observed")) +
    geom_line(aes(y=fmean, color="filtered")) + 
    geom_line(aes(y=smean, color="smoothed")) +
    geom_point(data=filter(df, flag), aes(color="outliers")) +
    facet_wrap_paginate(.~county,ncol=5,scales = 'free_y') + 
    theme_cowplot()  + xlab("date")  +
    ylab(attributes(df)$metadata$signal) +
    scale_fill_manual(breaks=c("filtered","smoothed"), 
                      values=c("blue","darkgreen"))+
    scale_color_manual(
      breaks=c("observed", "filtered","smoothed","outliers"),
      values = c("black", "blue","darkgreen","red")) +
    ggtitle(paste0("Counties with Outliers"))
            
fixed_p <- set_panel_size(p,width  = unit(6, "cm"),height = unit(4, "cm"))
  grid.newpage(recording = TRUE)

  grid.draw(fixed_p)  
```


#Outliers' table
```{r}
outliers <-  df %>% group_by(county) %>% filter(flag == 'TRUE') %>%
 select(county,signal,time_value, value, fmean, fsd, smean, ssd) %>%
  arrange(time_value)

datatable(outliers,options = list(scrollX = TRUE, scrollY = "600px",paging = FALSE),rownames = NULL) %>% formatSignif(~fmean+fsd+smean+ssd,3)
```


## Make corrections

Now we use the "multinomial" smoother to backfill the excess of any flagged outliers. Some notes:

* We use a new function `corrections_multinom_roll()` to do the backfill.
* It backfills deterministicly rather than randomly.
* It rounds alternate days up or down to try to avoid too much integers such that the sum is the excess.
* Optionally allows for filling non-uniformly.

# Treat all countys the same way

```{r}
corrected_counties <- county_filtered %>% mutate(
   FIPS = as.numeric(geo_value),
   excess = value - na_replace(smedian, fmedian),
   excess = floor(excess - excess_cut*sign(excess)*na_replace(smad,fmad)),
   corrected = corrections_multinom_roll(
     value, excess, flag, time_value, FIPS, backfill_lag))
```

# Treat RI and VA specially
```{r}
corrected_counties_2 <- county_filtered %>% 
  mutate(
    FIPS = as.numeric(geo_value),
    excess = value - na_replace(smedian, fmedian),
    excess = floor(excess - excess_cut*sign(excess)*na_replace(smad,fmad)),
    flag_bad_RI = (geo_value == "44007"  && lag(value) > 0),
    corrected = corrections_multinom_roll(
      value, value, flag_bad_RI, time_value, FIPS, 7),
    flag_bad_VA =(geo_value == "51059"  && lag(value) > 0),
    corrected = corrections_multinom_roll(
      value, value, flag_bad_VA, time_value, FIPS, 7),
    corrected = corrections_multinom_roll(
      corrected, excess, flag &!flag_bad_RI &! flag_bad_VA, time_value, 
      FIPS, backfill_lag))
```



# Visualize corrected counties
```{r show-corrections, fig.height = 80, fig.width = 20}
corrected_counties %>%
  dplyr::select(geo_value, time_value, value, corrected, flag) %>%
  pivot_longer(value:corrected) %>%
  ggplot(aes(time_value))+geom_line(aes(y=value, color=name))+
  geom_point(
    data = filter(corrected_counties, flag), aes(y=value), color="red")+ 
  facet_wrap(~geo_value, scales = "free_y", ncol = 5)+
  theme_cowplot()+xlab("date")+
  ylab(attributes(county_filtered)$metadata$signal)+
  scale_color_viridis_d()
```


## Show all corrected time points
```{r}
vectorized_ifelse <- function(test, yes, no){
  tmp = yes
  tmp[!test] = no[!test]
  tmp
}
sum_check = corrected_counties %>%
  summarise(original = sum(value, na.rm=TRUE),
            corrected = sum(corrected, na.rm = TRUE)) %>% ungroup() %>%
  mutate(diffs = abs(original-corrected)) %>%
  summarise(trouble = any(diffs != 0L))
print(paste0("Any totals changed? ", sum_check))

tosave = corrected_counties %>% 
  mutate(output = abs(corrected - value) > 0) %>%
  filter(output) %>% ungroup() %>%
  dplyr::select(geo_value, time_value, value, corrected) %>%
  transmute(
    location = as.character(geo_value), 
    value = corrected,
    variable_name = "usa-facts_confirmed_incidence_num",
    #location = as.numeric(STATE_TO_FIPS[location_name]),
    reference_date = time_value) %>%
  filter(reference_date >= params$outlier_start_date) %>%
  dplyr::select(location, reference_date,
                variable_name, value)

corrected_counties %>% 
  mutate(output = abs(corrected - value) > 0) %>%
  filter(output) %>%
  dplyr::select(geo_value, time_value, value, corrected) %>%
  transmute(time_value=time_value, 
            geo_value=geo_value,
            orig_value = value,
            replacement = corrected
            ) %>%
  datatable(
    options = list(scrollX = TRUE, scrollY = "300px",paging = FALSE),
    rownames = NULL) 
```


```{r eval=params$write_RDS}
saveRDS(tosave, file="county_corrections_djm.RDS")
```



